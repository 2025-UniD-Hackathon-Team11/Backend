"""
RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤
ê°•ì˜ë³„ í˜ë¥´ì†Œë‚˜ì™€ ë¬¸ì„œë¥¼ í™œìš©í•œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
RTX 2060 ìµœì í™” - ê²½ëŸ‰ ì„ë² ë”© ëª¨ë¸ + FAISS ë²¡í„° ê²€ìƒ‰
"""

import json
import os
from typing import List, Dict, Optional, Tuple
import numpy as np
from pathlib import Path
import pickle
import torch


class RAGService:
    """ê°•ì˜ë³„ RAG ì„œë¹„ìŠ¤ - RTX 2060 ìµœì í™”"""
    
    def __init__(self, lecture_id: int, base_dir: str = None):
        """
        Args:
            lecture_id: ê°•ì˜ ID
            base_dir: ë°ì´í„° ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: app/data)
        """
        self.lecture_id = lecture_id
        
        if base_dir is None:
            current_dir = Path(__file__).parent.parent
            base_dir = current_dir / "data"
        
        self.base_dir = Path(base_dir)
        self.llm_dir = self.base_dir / str(lecture_id) / "llm"
        self.documents_dir = self.llm_dir / "documents"
        self.embeddings_dir = self.llm_dir / "embeddings"
        self.persona_file = self.llm_dir / "persona.json"
        
        # ì„ë² ë”© ëª¨ë¸ (lazy loading)
        self._embedding_model = None
        self._faiss_index = None
        self._section_index = None  # ì„¹ì…˜ ì œëª© ì¸ë±ìŠ¤
        self._chunks_metadata = []
        self._sections = []  # ì„¹ì…˜ ë°ì´í„°
        
        # í˜ë¥´ì†Œë‚˜ ë¡œë“œ
        self.persona = self._load_persona()
        
        # ì„ë² ë”© ëª¨ë¸ ì¦‰ì‹œ ë¡œë“œ (ì¸ë±ìŠ¤ ë¹Œë“œ ì „ì— ë¨¼ì €!)
        self._load_embedding_model()
        
        # ì„¹ì…˜ ê¸°ë°˜ ë¬¸ì„œ ë¡œë“œ
        self._load_sections()
        
        # ë¬¸ì„œ ë¡œë“œ ë° ì¸ë±ìŠ¤ ì¤€ë¹„
        self.documents = []
        self._load_documents()
        self._load_or_build_index()
    
    def _load_embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì¦‰ì‹œ ë¡œë“œ"""
        if self._embedding_model is None:
            from sentence_transformers import SentenceTransformer
            self._embedding_model = SentenceTransformer(
                'paraphrase-multilingual-MiniLM-L12-v2',
                device='cuda' if torch.cuda.is_available() else 'cpu'
            )
    
    @property
    def embedding_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë°˜í™˜"""
        return self._embedding_model
    
    def _load_persona(self) -> Dict:
        """í˜ë¥´ì†Œë‚˜ ì„¤ì • ë¡œë“œ"""
        if not self.persona_file.exists():
            raise FileNotFoundError(f"í˜ë¥´ì†Œë‚˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.persona_file}")
        
        with open(self.persona_file, "r", encoding="utf-8") as f:
            return json.load(f)
    
    def _load_sections(self):
        """ì„¹ì…˜ ë°ì´í„° ë¡œë“œ (sections.json)"""
        sections_file = self.documents_dir / "sections.json"
        
        if sections_file.exists():
            with open(sections_file, "r", encoding="utf-8") as f:
                self._sections = json.load(f)
        else:
            self._sections = []
    
    def _load_documents(self):
        """ë¬¸ì„œ ë¡œë“œ"""
        if self.documents_dir.exists():
            for doc_file in self.documents_dir.glob("*.txt"):
                with open(doc_file, "r", encoding="utf-8") as f:
                    content = f.read()
                    self.documents.append({
                        "filename": doc_file.name,
                        "content": content
                    })
    
    def _split_into_chunks(self, text: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (RTX 2060 ìµœì í™”: ì‘ì€ ì²­í¬)
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜) - 300ìë¡œ ì¤„ì„
            overlap: ì²­í¬ ê°„ ì˜¤ë²„ë© í¬ê¸°
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        chunks = []
        
        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„í• 
        paragraphs = text.split('\n\n')
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            if len(para) <= chunk_size:
                chunks.append(para)
            else:
                # ê¸´ ë¬¸ë‹¨ì€ chunk_sizeë¡œ ë¶„í• 
                start = 0
                while start < len(para):
                    end = start + chunk_size
                    chunk = para[start:end]
                    chunks.append(chunk.strip())
                    start += chunk_size - overlap
        
        return chunks
    
    def _load_or_build_index(self):
        """FAISS ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±"""
        index_file = self.embeddings_dir / "faiss_index.bin"
        metadata_file = self.embeddings_dir / "chunks_metadata.pkl"
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ë¡œë“œ
        if index_file.exists() and metadata_file.exists():
            try:
                import faiss
                self._faiss_index = faiss.read_index(str(index_file))
                with open(metadata_file, "rb") as f:
                    self._chunks_metadata = pickle.load(f)
                return
            except Exception:
                pass
        
        # ì¸ë±ìŠ¤ ìƒˆë¡œ ìƒì„±
        self._build_index()
    
    def _build_index(self):
        """FAISS ì¸ë±ìŠ¤ ìƒì„± (ê³„ì¸µì : ì„¹ì…˜ ì œëª© + ë‚´ìš©)"""
        if self._sections:
            self._build_hierarchical_index()
        elif self.documents:
            self._build_flat_index()
    
    def _build_hierarchical_index(self):
        """ê³„ì¸µì  ì¸ë±ìŠ¤: 1) ì„¹ì…˜ ì œëª© ê²€ìƒ‰ â†’ 2) ê´€ë ¨ ì„¹ì…˜ ë‚´ìš© ë°˜í™˜"""
        import faiss
        
        # 1ë‹¨ê³„: ì„¹ì…˜ ì œëª© ì„ë² ë”©
        section_titles = [s["title"] for s in self._sections]
        title_embeddings = self.embedding_model.encode(
            section_titles,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # ì„¹ì…˜ ì œëª© ì¸ë±ìŠ¤ ìƒì„±
        dimension = title_embeddings.shape[1]
        self._section_index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(title_embeddings)
        self._section_index.add(title_embeddings)
        
        # 2ë‹¨ê³„: ì„¹ì…˜ ë©”íƒ€ë°ì´í„° ì €ì¥
        self._chunks_metadata = []
        for section in self._sections:
            self._chunks_metadata.append({
                "text": section["content"],
                "title": section["title"],
                "source": f"Section: {section['title']}",
                "page": section.get("page", 0)
            })
        
        # ì¸ë±ìŠ¤ ì €ì¥
        self.embeddings_dir.mkdir(parents=True, exist_ok=True)
        faiss.write_index(self._section_index, str(self.embeddings_dir / "section_index.bin"))
        with open(self.embeddings_dir / "sections_metadata.pkl", "wb") as f:
            pickle.dump(self._chunks_metadata, f)
    
    def _build_flat_index(self):
        """ì¼ë°˜ ì¸ë±ìŠ¤: ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ê²€ìƒ‰"""
        import faiss
        
        # ëª¨ë“  ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
        all_chunks = []
        for doc in self.documents:
            chunks = self._split_into_chunks(doc["content"])
            for chunk in chunks:
                all_chunks.append({
                    "text": chunk,
                    "source": doc["filename"]
                })
        
        if not all_chunks:
            return
        
        self._chunks_metadata = all_chunks
        
        # ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
        texts = [chunk["text"] for chunk in all_chunks]
        embeddings = self.embedding_model.encode(
            texts,
            batch_size=32,  # RTX 2060 ìµœì í™”
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„± (IndexFlatIP: ë‚´ì  ê¸°ë°˜ ìœ ì‚¬ë„)
        dimension = embeddings.shape[1]
        self._faiss_index = faiss.IndexFlatIP(dimension)
        
        # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©)
        faiss.normalize_L2(embeddings)
        self._faiss_index.add(embeddings)
        
        # ì¸ë±ìŠ¤ ì €ì¥
        self.embeddings_dir.mkdir(parents=True, exist_ok=True)
        faiss.write_index(self._faiss_index, str(self.embeddings_dir / "faiss_index.bin"))
        with open(self.embeddings_dir / "chunks_metadata.pkl", "wb") as f:
            pickle.dump(self._chunks_metadata, f)
    
    def rebuild_index(self):
        """ì¸ë±ìŠ¤ ê°•ì œ ì¬ìƒì„± (ë¬¸ì„œ ì¶”ê°€/ìˆ˜ì • ì‹œ í˜¸ì¶œ)"""
        self._build_index()
    
    def get_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        return self.persona.get("system_prompt", "")
    
    def get_persona_info(self) -> Dict:
        """í˜ë¥´ì†Œë‚˜ ì •ë³´ ë°˜í™˜"""
        return self.persona.get("persona", {})
    
    def add_document(self, filename: str, content: str) -> bool:
        """
        ìƒˆ ë¬¸ì„œ ì¶”ê°€
        
        Args:
            filename: íŒŒì¼ëª…
            content: ë¬¸ì„œ ë‚´ìš©
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ë¬¸ì„œ ì €ì¥
            doc_path = self.documents_dir / filename
            self.documents_dir.mkdir(parents=True, exist_ok=True)
            
            with open(doc_path, "w", encoding="utf-8") as f:
                f.write(content)
            
            # ë©”ëª¨ë¦¬ì—ë„ ì¶”ê°€
            self.documents.append({
                "filename": filename,
                "content": content
            })
            
            # ì¸ë±ìŠ¤ ì¬ìƒì„±
            self.rebuild_index()
            
            return True
        except Exception as e:
            print(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def retrieve_relevant_chunks(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¬¸ì„œ ì²­í¬ ê²€ìƒ‰ (ê³„ì¸µì  ê²€ìƒ‰)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
        
        Returns:
            ê´€ë ¨ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ê° ì²­í¬ëŠ” {'content', 'score', 'source', 'title'} í¬í•¨)
        """
        # ê³„ì¸µì  ê²€ìƒ‰: ì„¹ì…˜ ì œëª© ê¸°ë°˜
        if self._section_index is not None:
            return self._hierarchical_search(query, top_k)
        # ì¼ë°˜ ê²€ìƒ‰: ì²­í¬ ê¸°ë°˜ (fallback)
        elif self._faiss_index is not None:
            return self._flat_search(query, top_k)
        else:
            return []
    
    def _hierarchical_search(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        ê³„ì¸µì  ê²€ìƒ‰: 1) ì œëª©ìœ¼ë¡œ ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸° â†’ 2) ì„¹ì…˜ ì „ì²´ ë‚´ìš© ë°˜í™˜
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ì„¹ì…˜ ìˆ˜
        
        Returns:
            ê´€ë ¨ ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
        """
        import faiss
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode(
            [query],
            convert_to_numpy=True
        )
        faiss.normalize_L2(query_embedding)
        
        # ì„¹ì…˜ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰
        scores, indices = self._section_index.search(query_embedding, min(top_k, len(self._chunks_metadata)))
        
        results = []
        print(f"\nğŸ” ê³„ì¸µì  RAG ê²€ìƒ‰ (Heading 2 ê¸°ë°˜)")
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self._chunks_metadata):
                section = self._chunks_metadata[idx]
                results.append({
                    "content": section["text"],
                    "title": section["title"],
                    "score": float(score),
                    "source": section["source"]
                })
                print(f"  âœ“ [{float(score):.3f}] {section['title']}")
        
        return results
    
    def _flat_search(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        ì¼ë°˜ ê²€ìƒ‰: ëª¨ë“  ì²­í¬ì—ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ì²­í¬ ìˆ˜
        
        Returns:
            ê´€ë ¨ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        import faiss
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode(
            [query],
            convert_to_numpy=True
        )
        faiss.normalize_L2(query_embedding)
        
        # FAISS ê²€ìƒ‰
        scores, indices = self._faiss_index.search(query_embedding, top_k)
        
        # ê²°ê³¼ êµ¬ì„±
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self._chunks_metadata):
                chunk_meta = self._chunks_metadata[idx]
                results.append({
                    "content": chunk_meta["text"],
                    "score": float(score),
                    "source": chunk_meta["source"]
                })
        
        return results
    
    def build_rag_context(self, query: str, top_k: int = 3) -> str:
        """
        RAGë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ì²­í¬ ê°œìˆ˜
        
        Returns:
            LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        relevant_chunks = self.retrieve_relevant_chunks(query, top_k)
        
        if not relevant_chunks:
            return "ê´€ë ¨ ê°•ì˜ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = ["ë‹¤ìŒì€ ê´€ë ¨ëœ ê°•ì˜ ìë£Œì…ë‹ˆë‹¤:\n"]
        
        for i, chunk in enumerate(relevant_chunks, 1):
            context_parts.append(f"\n[ì°¸ê³ ìë£Œ {i}] (ì¶œì²˜: {chunk['source']}, ê´€ë ¨ë„: {chunk['score']:.2f})")
            context_parts.append(chunk['content'])
            context_parts.append("")
        
        return "\n".join(context_parts)
    
    def prepare_llm_request(self, user_question: str, top_k: int = 3) -> Dict:
        """
        LLM ìš”ì²­ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        
        Args:
            user_question: ì‚¬ìš©ì ì§ˆë¬¸
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ì²­í¬ ê°œìˆ˜
        
        Returns:
            LLM ìš”ì²­ ë°ì´í„° (system_prompt, context, question ë“±)
        """
        system_prompt = self.get_system_prompt()
        context = self.build_rag_context(user_question, top_k)
        
        return {
            "system_prompt": system_prompt,
            "context": context,
            "question": user_question,
            "persona": self.get_persona_info(),
            "temperature": self.persona.get("temperature", 0.7),
            "max_tokens": self.persona.get("max_tokens", 1000)
        }


def create_rag_service(lecture_id: int) -> RAGService:
    """
    RAG ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
    Args:
        lecture_id: ê°•ì˜ ID
    
    Returns:
        RAGService ì¸ìŠ¤í„´ìŠ¤
    """
    return RAGService(lecture_id)


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê°•ì˜ 1ë²ˆì˜ RAG ì„œë¹„ìŠ¤ ìƒì„±
    print("ğŸš€ RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    rag = create_rag_service(lecture_id=1)
    
    # í˜ë¥´ì†Œë‚˜ ì •ë³´ í™•ì¸
    print("\n=== í˜ë¥´ì†Œë‚˜ ì •ë³´ ===")
    print(json.dumps(rag.get_persona_info(), indent=2, ensure_ascii=False))
    
    # ì§ˆë¬¸ ì˜ˆì‹œ
    questions = [
        "ì»¨ë³¼ë£¨ì…˜ì´ ë­”ê°€ìš”?",
        "ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?",
        "CNNì—ì„œ paddingì€ ì™œ ì‚¬ìš©í•˜ë‚˜ìš”?"
    ]
    
    for question in questions:
        print(f"\n{'='*60}")
        print(f"ì§ˆë¬¸: {question}")
        print("="*60)
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        chunks = rag.retrieve_relevant_chunks(question, top_k=2)
        
        print("\nê²€ìƒ‰ ê²°ê³¼:")
        for i, chunk in enumerate(chunks, 1):
            print(f"\n[{i}] ì¶œì²˜: {chunk['source']} (ì ìˆ˜: {chunk['score']:.3f})")
            print(f"ë‚´ìš©: {chunk['content'][:150]}...")
        
        # LLM ìš”ì²­ ë°ì´í„° ì¤€ë¹„
        llm_request = rag.prepare_llm_request(question, top_k=2)
        print(f"\nâœ… LLM ìš”ì²­ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
